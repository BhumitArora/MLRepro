{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7555875,"sourceType":"datasetVersion","datasetId":4400476}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-09T12:16:27.078819Z","iopub.execute_input":"2024-02-09T12:16:27.079480Z","iopub.status.idle":"2024-02-09T12:16:27.440825Z","shell.execute_reply.started":"2024-02-09T12:16:27.079440Z","shell.execute_reply":"2024-02-09T12:16:27.440079Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nprint(sys.version)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:16:28.665394Z","iopub.execute_input":"2024-02-09T12:16:28.666200Z","iopub.status.idle":"2024-02-09T12:16:28.671107Z","shell.execute_reply.started":"2024-02-09T12:16:28.666168Z","shell.execute_reply":"2024-02-09T12:16:28.670017Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:16:29.587337Z","iopub.execute_input":"2024-02-09T12:16:29.587929Z","iopub.status.idle":"2024-02-09T12:16:30.547915Z","shell.execute_reply.started":"2024-02-09T12:16:29.587901Z","shell.execute_reply":"2024-02-09T12:16:30.546794Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\nCuda compilation tools, release 12.1, V12.1.105\nBuild cuda_12.1.r12.1/compiler.32688072_0\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install torch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:16:30.550025Z","iopub.execute_input":"2024-02-09T12:16:30.550410Z","iopub.status.idle":"2024-02-09T12:18:49.012594Z","shell.execute_reply.started":"2024-02-09T12:16:30.550377Z","shell.execute_reply":"2024-02-09T12:18:49.011181Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting torch==2.1.0\n  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\nCollecting torchvision==0.16.0\n  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting torchaudio==2.1.0\n  Downloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl.metadata (5.7 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.0) (2023.12.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0)\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.0) (1.24.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.16.0) (9.5.0)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0)\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision==0.16.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.0) (1.3.0)\nDownloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchaudio-2.1.0-cp310-cp310-manylinux1_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m952.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.16.2\n    Uninstalling torchvision-0.16.2:\n      Successfully uninstalled torchvision-0.16.2\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.1.2\n    Uninstalling torchaudio-2.1.2:\n      Successfully uninstalled torchaudio-2.1.2\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchaudio-2.1.0 torchvision-0.16.0 triton-2.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/BhumitArora/SLaKtemp.git","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:18:49.014995Z","iopub.execute_input":"2024-02-09T12:18:49.015726Z","iopub.status.idle":"2024-02-09T12:18:52.208730Z","shell.execute_reply.started":"2024-02-09T12:18:49.015683Z","shell.execute_reply":"2024-02-09T12:18:52.207675Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Cloning into 'SLaKtemp'...\nremote: Enumerating objects: 5504, done.\u001b[K\nremote: Counting objects: 100% (816/816), done.\u001b[K (449/816)\u001b[K\nremote: Compressing objects: 100% (601/601), done.\u001b[K\nremote: Total 5504 (delta 270), reused 698 (delta 197), pack-reused 4688\u001b[K\nReceiving objects: 100% (5504/5504), 27.73 MiB | 35.81 MiB/s, done.\nResolving deltas: 100% (2901/2901), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:18:52.210243Z","iopub.execute_input":"2024-02-09T12:18:52.210546Z","iopub.status.idle":"2024-02-09T12:18:54.297187Z","shell.execute_reply.started":"2024-02-09T12:18:52.210519Z","shell.execute_reply":"2024-02-09T12:18:54.296204Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2.1.0+cu121\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install timm tensorboardX six","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:18:54.298963Z","iopub.execute_input":"2024-02-09T12:18:54.299394Z","iopub.status.idle":"2024-02-09T12:19:06.538855Z","shell.execute_reply.started":"2024-02-09T12:18:54.299366Z","shell.execute_reply":"2024-02-09T12:19:06.537668Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.12)\nRequirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (2.6.2.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (1.16.0)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.20.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (1.24.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (21.3)\nRequirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.10/site-packages (from tensorboardX) (3.20.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2023.12.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2.18.1)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (12.1.105)\nRequirement already satisfied: triton==2.1.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.7->timm) (2.1.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.7->timm) (12.3.101)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->timm) (4.66.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorboardX) (3.1.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.7->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->timm) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"!mkdir /kaggle/working/Dataset/","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:19:06.540346Z","iopub.execute_input":"2024-02-09T12:19:06.540659Z","iopub.status.idle":"2024-02-09T12:19:07.519494Z","shell.execute_reply.started":"2024-02-09T12:19:06.540633Z","shell.execute_reply":"2024-02-09T12:19:07.518251Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"!pip install /kaggle/working/SLaKtemp/cutlass/examples/19_large_depthwise_conv2d_torch_extension/.","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:19:07.521177Z","iopub.execute_input":"2024-02-09T12:19:07.522022Z","iopub.status.idle":"2024-02-09T12:23:08.238608Z","shell.execute_reply.started":"2024-02-09T12:19:07.521982Z","shell.execute_reply":"2024-02-09T12:23:08.237395Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Processing ./SLaKtemp/cutlass/examples/19_large_depthwise_conv2d_torch_extension\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: depthwise_conv2d_implicit_gemm\n  Building wheel for depthwise_conv2d_implicit_gemm (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for depthwise_conv2d_implicit_gemm: filename=depthwise_conv2d_implicit_gemm-0.0.0-cp310-cp310-linux_x86_64.whl size=9730725 sha256=848536f9e924e45749d9e3dcf5c5788e524d94b28f17560c1e875bd846a2a3a4\n  Stored in directory: /tmp/pip-ephem-wheel-cache-whv4x__p/wheels/22/78/0c/2eee0d5f573c6bb172328b8600f76ac8c5851ea37b634a87bb\nSuccessfully built depthwise_conv2d_implicit_gemm\nInstalling collected packages: depthwise_conv2d_implicit_gemm\nSuccessfully installed depthwise_conv2d_implicit_gemm-0.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!python /kaggle/working/SLaKtemp/depthwise_conv2d_implicit_gemm.py","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:23:08.240314Z","iopub.execute_input":"2024-02-09T12:23:08.240667Z","iopub.status.idle":"2024-02-09T12:23:13.635632Z","shell.execute_reply.started":"2024-02-09T12:23:08.240637Z","shell.execute_reply":"2024-02-09T12:23:13.634625Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"output difference: tensor(2.3746e-08, device='cuda:0', grad_fn=<MeanBackward0>)\ngradient difference: tensor(1.8905e-11, device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"import sys\nsys.path.insert(1, \"/kaggle/working/SLaKtemp/cutlass/examples/19_large_depthwise_conv2d_torch_extension\")","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:38:07.430794Z","iopub.execute_input":"2024-02-09T12:38:07.431725Z","iopub.status.idle":"2024-02-09T12:38:07.436735Z","shell.execute_reply.started":"2024-02-09T12:38:07.431687Z","shell.execute_reply":"2024-02-09T12:38:07.435736Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from depthwise_conv2d_implicit_gemm import DepthWiseConv2dImplicitGEMM","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:38:14.433896Z","iopub.execute_input":"2024-02-09T12:38:14.434778Z","iopub.status.idle":"2024-02-09T12:38:15.386002Z","shell.execute_reply.started":"2024-02-09T12:38:14.434745Z","shell.execute_reply":"2024-02-09T12:38:15.384965Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!export LARGE_KERNEL_CONV_IMPL=/kaggle/working/SLaKtemp/cutlass/examples/19_large_depthwise_conv2d_torch_extension\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:38:17.855915Z","iopub.execute_input":"2024-02-09T12:38:17.856313Z","iopub.status.idle":"2024-02-09T12:38:18.805808Z","shell.execute_reply.started":"2024-02-09T12:38:17.856283Z","shell.execute_reply":"2024-02-09T12:38:18.804692Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!python /kaggle/working/SLaKtemp/main.py --model SLaK_base --eval true \\\n--Decom True --kernel_size 51 49 47 13 5 --width_factor 1.3 \\\n--resume /kaggle/input/slakbase-weights/checkpoint-best-ema_slakbaseema.pth \\\n--input_size 32 --drop_path 0.2 \\\n--data_path /kaggle/working/Dataset --data_set 'CIFAR'","metadata":{"execution":{"iopub.status.busy":"2024-02-09T12:38:21.030491Z","iopub.execute_input":"2024-02-09T12:38:21.030864Z","iopub.status.idle":"2024-02-09T12:39:34.360290Z","shell.execute_reply.started":"2024-02-09T12:38:21.030836Z","shell.execute_reply":"2024-02-09T12:39:34.359093Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Not using distributed mode\nNamespace(batch_size=64, epochs=300, update_freq=1, model='SLaK_base', drop_path=0.2, input_size=32, layer_scale_init_value=1e-06, model_ema=False, model_ema_decay=0.9999, model_ema_force_cpu=False, model_ema_eval=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, weight_decay_end=None, lr=0.004, layer_decay=1.0, min_lr=1e-06, warmup_epochs=20, warmup_steps=-1, color_jitter=0.4, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', crop_pct=None, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', finetune='', head_init_scale=1.0, model_key='model|module', model_prefix='', data_path='/kaggle/working/Dataset', eval_data_path=None, nb_classes=1000, imagenet_default_mean_and_std=True, data_set='CIFAR', output_dir='', log_dir=None, device='cuda', seed=0, resume='/kaggle/input/slakbase-weights/checkpoint-best-ema_slakbaseema.pth', auto_resume=True, save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, start_epoch=0, eval=True, dist_eval=True, disable_eval=False, num_workers=10, pin_mem=True, world_size=1, local_rank=-1, dist_on_itp=False, dist_url='env://', use_amp=False, enable_wandb=False, project='SLaK', wandb_ckpt=False, Decom=True, width_factor=1.3, sparse=False, kernel_size=[51, 49, 47, 13, 5], growth='random', prune='magnitude', redistribution='none', prune_rate=0.3, sparsity=0.4, verbose=False, fix=False, sparse_init='snip', update_frequency=100, only_L=False, bn=True, distributed=False)\nTransform = \nRandomCrop(size=(32, 32), padding=4)\nRandomHorizontalFlip(p=0.5)\nRandAugment(n=2, ops=\n\tAugmentOp(name=AutoContrast, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=Equalize, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=Invert, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=Rotate, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=PosterizeIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=SolarizeIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=SolarizeAdd, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=ColorIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=ContrastIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=BrightnessIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=SharpnessIncreasing, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=ShearX, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=ShearY, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=TranslateXRel, p=0.5, m=9, mstd=0.5)\n\tAugmentOp(name=TranslateYRel, p=0.5, m=9, mstd=0.5))\nToTensor()\nNormalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\nRandomErasing(p=0.25, mode=pixel, count=(1, 1))\n---------------------------\nDownloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /kaggle/working/Dataset/cifar-100-python.tar.gz\n100%|███████████████████████| 169001437/169001437 [00:03<00:00, 49361539.31it/s]\nExtracting /kaggle/working/Dataset/cifar-100-python.tar.gz to /kaggle/working/Dataset\nNumber of the class = 100\nTransform = \nToTensor()\nNormalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n---------------------------\nFiles already downloaded and verified\nNumber of the class = 100\nSampler_train = <torch.utils.data.distributed.DistributedSampler object at 0x7d88e9608f70>\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nMixup is activated!\nModel = SLaK(\n  (downsample_layers): ModuleList(\n    (0): Sequential(\n      (0): Conv2d(3, 166, kernel_size=(4, 4), stride=(4, 4))\n      (1): LayerNorm()\n    )\n    (1): Sequential(\n      (0): LayerNorm()\n      (1): Conv2d(166, 332, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (2): Sequential(\n      (0): LayerNorm()\n      (1): Conv2d(332, 665, kernel_size=(2, 2), stride=(2, 2))\n    )\n    (3): Sequential(\n      (0): LayerNorm()\n      (1): Conv2d(665, 1331, kernel_size=(2, 2), stride=(2, 2))\n    )\n  )\n  (stages): ModuleList(\n    (0): Sequential(\n      (0): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(51, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 51), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=166, out_features=664, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=664, out_features=166, bias=True)\n        (drop_path): Identity()\n      )\n      (1): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(51, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 51), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=166, out_features=664, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=664, out_features=166, bias=True)\n        (drop_path): DropPath(drop_prob=0.006)\n      )\n      (2): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(51, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 51), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(166, 166, kernel_size=(5, 5), stride=(1, 1), groups=166, bias=False)\n            (bn): SyncBatchNorm(166, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=166, out_features=664, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=664, out_features=166, bias=True)\n        (drop_path): DropPath(drop_prob=0.011)\n      )\n    )\n    (1): Sequential(\n      (0): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(49, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 49), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=332, out_features=1328, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=1328, out_features=332, bias=True)\n        (drop_path): DropPath(drop_prob=0.017)\n      )\n      (1): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(49, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 49), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=332, out_features=1328, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=1328, out_features=332, bias=True)\n        (drop_path): DropPath(drop_prob=0.023)\n      )\n      (2): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(49, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 49), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(332, 332, kernel_size=(5, 5), stride=(1, 1), groups=332, bias=False)\n            (bn): SyncBatchNorm(332, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=332, out_features=1328, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=1328, out_features=332, bias=True)\n        (drop_path): DropPath(drop_prob=0.029)\n      )\n    )\n    (2): Sequential(\n      (0): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.034)\n      )\n      (1): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.040)\n      )\n      (2): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.046)\n      )\n      (3): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.051)\n      )\n      (4): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.057)\n      )\n      (5): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.063)\n      )\n      (6): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.069)\n      )\n      (7): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.074)\n      )\n      (8): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.080)\n      )\n      (9): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.086)\n      )\n      (10): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.091)\n      )\n      (11): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.097)\n      )\n      (12): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.103)\n      )\n      (13): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.109)\n      )\n      (14): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.114)\n      )\n      (15): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.120)\n      )\n      (16): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.126)\n      )\n      (17): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.131)\n      )\n      (18): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.137)\n      )\n      (19): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.143)\n      )\n      (20): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.149)\n      )\n      (21): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.154)\n      )\n      (22): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.160)\n      )\n      (23): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.166)\n      )\n      (24): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.171)\n      )\n      (25): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.177)\n      )\n      (26): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(47, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 47), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(665, 665, kernel_size=(5, 5), stride=(1, 1), groups=665, bias=False)\n            (bn): SyncBatchNorm(665, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=665, out_features=2660, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=2660, out_features=665, bias=True)\n        (drop_path): DropPath(drop_prob=0.183)\n      )\n    )\n    (3): Sequential(\n      (0): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(13, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 13), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=1331, out_features=5324, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=5324, out_features=1331, bias=True)\n        (drop_path): DropPath(drop_prob=0.189)\n      )\n      (1): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(13, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 13), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=1331, out_features=5324, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=5324, out_features=1331, bias=True)\n        (drop_path): DropPath(drop_prob=0.194)\n      )\n      (2): Block(\n        (large_kernel): ReparamLargeKernelConv(\n          (LoRA1): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(13, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (LoRA2): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 13), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n          (small_conv): Sequential(\n            (conv): DepthWiseConv2dImplicitGEMM(1331, 1331, kernel_size=(5, 5), stride=(1, 1), groups=1331, bias=False)\n            (bn): SyncBatchNorm(1331, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          )\n        )\n        (norm): LayerNorm()\n        (pwconv1): Linear(in_features=1331, out_features=5324, bias=True)\n        (act): GELU(approximate='none')\n        (pwconv2): Linear(in_features=5324, out_features=1331, bias=True)\n        (drop_path): DropPath(drop_prob=0.200)\n      )\n    )\n  )\n  (norm): LayerNorm((1331,), eps=1e-06, elementwise_affine=True)\n  (head): Linear(in_features=1331, out_features=100, bias=True)\n)\nnumber of params: 156751992\nLR = 0.00400000\nBatch size = 64\nUpdate frequent = 1\nNumber of training examples = 50000\nNumber of training steps per epoch = 781\nParam groups = {\n  \"decay\": {\n    \"weight_decay\": 0.05,\n    \"params\": [\n      \"downsample_layers.0.0.weight\",\n      \"downsample_layers.1.1.weight\",\n      \"downsample_layers.2.1.weight\",\n      \"downsample_layers.3.1.weight\",\n      \"stages.0.0.large_kernel.LoRA1.conv.weight\",\n      \"stages.0.0.large_kernel.LoRA2.conv.weight\",\n      \"stages.0.0.large_kernel.small_conv.conv.weight\",\n      \"stages.0.0.pwconv1.weight\",\n      \"stages.0.0.pwconv2.weight\",\n      \"stages.0.1.large_kernel.LoRA1.conv.weight\",\n      \"stages.0.1.large_kernel.LoRA2.conv.weight\",\n      \"stages.0.1.large_kernel.small_conv.conv.weight\",\n      \"stages.0.1.pwconv1.weight\",\n      \"stages.0.1.pwconv2.weight\",\n      \"stages.0.2.large_kernel.LoRA1.conv.weight\",\n      \"stages.0.2.large_kernel.LoRA2.conv.weight\",\n      \"stages.0.2.large_kernel.small_conv.conv.weight\",\n      \"stages.0.2.pwconv1.weight\",\n      \"stages.0.2.pwconv2.weight\",\n      \"stages.1.0.large_kernel.LoRA1.conv.weight\",\n      \"stages.1.0.large_kernel.LoRA2.conv.weight\",\n      \"stages.1.0.large_kernel.small_conv.conv.weight\",\n      \"stages.1.0.pwconv1.weight\",\n      \"stages.1.0.pwconv2.weight\",\n      \"stages.1.1.large_kernel.LoRA1.conv.weight\",\n      \"stages.1.1.large_kernel.LoRA2.conv.weight\",\n      \"stages.1.1.large_kernel.small_conv.conv.weight\",\n      \"stages.1.1.pwconv1.weight\",\n      \"stages.1.1.pwconv2.weight\",\n      \"stages.1.2.large_kernel.LoRA1.conv.weight\",\n      \"stages.1.2.large_kernel.LoRA2.conv.weight\",\n      \"stages.1.2.large_kernel.small_conv.conv.weight\",\n      \"stages.1.2.pwconv1.weight\",\n      \"stages.1.2.pwconv2.weight\",\n      \"stages.2.0.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.0.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.0.large_kernel.small_conv.conv.weight\",\n      \"stages.2.0.pwconv1.weight\",\n      \"stages.2.0.pwconv2.weight\",\n      \"stages.2.1.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.1.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.1.large_kernel.small_conv.conv.weight\",\n      \"stages.2.1.pwconv1.weight\",\n      \"stages.2.1.pwconv2.weight\",\n      \"stages.2.2.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.2.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.2.large_kernel.small_conv.conv.weight\",\n      \"stages.2.2.pwconv1.weight\",\n      \"stages.2.2.pwconv2.weight\",\n      \"stages.2.3.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.3.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.3.large_kernel.small_conv.conv.weight\",\n      \"stages.2.3.pwconv1.weight\",\n      \"stages.2.3.pwconv2.weight\",\n      \"stages.2.4.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.4.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.4.large_kernel.small_conv.conv.weight\",\n      \"stages.2.4.pwconv1.weight\",\n      \"stages.2.4.pwconv2.weight\",\n      \"stages.2.5.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.5.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.5.large_kernel.small_conv.conv.weight\",\n      \"stages.2.5.pwconv1.weight\",\n      \"stages.2.5.pwconv2.weight\",\n      \"stages.2.6.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.6.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.6.large_kernel.small_conv.conv.weight\",\n      \"stages.2.6.pwconv1.weight\",\n      \"stages.2.6.pwconv2.weight\",\n      \"stages.2.7.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.7.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.7.large_kernel.small_conv.conv.weight\",\n      \"stages.2.7.pwconv1.weight\",\n      \"stages.2.7.pwconv2.weight\",\n      \"stages.2.8.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.8.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.8.large_kernel.small_conv.conv.weight\",\n      \"stages.2.8.pwconv1.weight\",\n      \"stages.2.8.pwconv2.weight\",\n      \"stages.2.9.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.9.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.9.large_kernel.small_conv.conv.weight\",\n      \"stages.2.9.pwconv1.weight\",\n      \"stages.2.9.pwconv2.weight\",\n      \"stages.2.10.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.10.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.10.large_kernel.small_conv.conv.weight\",\n      \"stages.2.10.pwconv1.weight\",\n      \"stages.2.10.pwconv2.weight\",\n      \"stages.2.11.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.11.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.11.large_kernel.small_conv.conv.weight\",\n      \"stages.2.11.pwconv1.weight\",\n      \"stages.2.11.pwconv2.weight\",\n      \"stages.2.12.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.12.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.12.large_kernel.small_conv.conv.weight\",\n      \"stages.2.12.pwconv1.weight\",\n      \"stages.2.12.pwconv2.weight\",\n      \"stages.2.13.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.13.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.13.large_kernel.small_conv.conv.weight\",\n      \"stages.2.13.pwconv1.weight\",\n      \"stages.2.13.pwconv2.weight\",\n      \"stages.2.14.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.14.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.14.large_kernel.small_conv.conv.weight\",\n      \"stages.2.14.pwconv1.weight\",\n      \"stages.2.14.pwconv2.weight\",\n      \"stages.2.15.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.15.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.15.large_kernel.small_conv.conv.weight\",\n      \"stages.2.15.pwconv1.weight\",\n      \"stages.2.15.pwconv2.weight\",\n      \"stages.2.16.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.16.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.16.large_kernel.small_conv.conv.weight\",\n      \"stages.2.16.pwconv1.weight\",\n      \"stages.2.16.pwconv2.weight\",\n      \"stages.2.17.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.17.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.17.large_kernel.small_conv.conv.weight\",\n      \"stages.2.17.pwconv1.weight\",\n      \"stages.2.17.pwconv2.weight\",\n      \"stages.2.18.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.18.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.18.large_kernel.small_conv.conv.weight\",\n      \"stages.2.18.pwconv1.weight\",\n      \"stages.2.18.pwconv2.weight\",\n      \"stages.2.19.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.19.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.19.large_kernel.small_conv.conv.weight\",\n      \"stages.2.19.pwconv1.weight\",\n      \"stages.2.19.pwconv2.weight\",\n      \"stages.2.20.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.20.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.20.large_kernel.small_conv.conv.weight\",\n      \"stages.2.20.pwconv1.weight\",\n      \"stages.2.20.pwconv2.weight\",\n      \"stages.2.21.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.21.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.21.large_kernel.small_conv.conv.weight\",\n      \"stages.2.21.pwconv1.weight\",\n      \"stages.2.21.pwconv2.weight\",\n      \"stages.2.22.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.22.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.22.large_kernel.small_conv.conv.weight\",\n      \"stages.2.22.pwconv1.weight\",\n      \"stages.2.22.pwconv2.weight\",\n      \"stages.2.23.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.23.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.23.large_kernel.small_conv.conv.weight\",\n      \"stages.2.23.pwconv1.weight\",\n      \"stages.2.23.pwconv2.weight\",\n      \"stages.2.24.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.24.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.24.large_kernel.small_conv.conv.weight\",\n      \"stages.2.24.pwconv1.weight\",\n      \"stages.2.24.pwconv2.weight\",\n      \"stages.2.25.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.25.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.25.large_kernel.small_conv.conv.weight\",\n      \"stages.2.25.pwconv1.weight\",\n      \"stages.2.25.pwconv2.weight\",\n      \"stages.2.26.large_kernel.LoRA1.conv.weight\",\n      \"stages.2.26.large_kernel.LoRA2.conv.weight\",\n      \"stages.2.26.large_kernel.small_conv.conv.weight\",\n      \"stages.2.26.pwconv1.weight\",\n      \"stages.2.26.pwconv2.weight\",\n      \"stages.3.0.large_kernel.LoRA1.conv.weight\",\n      \"stages.3.0.large_kernel.LoRA2.conv.weight\",\n      \"stages.3.0.large_kernel.small_conv.conv.weight\",\n      \"stages.3.0.pwconv1.weight\",\n      \"stages.3.0.pwconv2.weight\",\n      \"stages.3.1.large_kernel.LoRA1.conv.weight\",\n      \"stages.3.1.large_kernel.LoRA2.conv.weight\",\n      \"stages.3.1.large_kernel.small_conv.conv.weight\",\n      \"stages.3.1.pwconv1.weight\",\n      \"stages.3.1.pwconv2.weight\",\n      \"stages.3.2.large_kernel.LoRA1.conv.weight\",\n      \"stages.3.2.large_kernel.LoRA2.conv.weight\",\n      \"stages.3.2.large_kernel.small_conv.conv.weight\",\n      \"stages.3.2.pwconv1.weight\",\n      \"stages.3.2.pwconv2.weight\",\n      \"head.weight\"\n    ],\n    \"lr_scale\": 1.0\n  },\n  \"no_decay\": {\n    \"weight_decay\": 0.0,\n    \"params\": [\n      \"downsample_layers.0.0.bias\",\n      \"downsample_layers.0.1.weight\",\n      \"downsample_layers.0.1.bias\",\n      \"downsample_layers.1.0.weight\",\n      \"downsample_layers.1.0.bias\",\n      \"downsample_layers.1.1.bias\",\n      \"downsample_layers.2.0.weight\",\n      \"downsample_layers.2.0.bias\",\n      \"downsample_layers.2.1.bias\",\n      \"downsample_layers.3.0.weight\",\n      \"downsample_layers.3.0.bias\",\n      \"downsample_layers.3.1.bias\",\n      \"stages.0.0.gamma\",\n      \"stages.0.0.large_kernel.LoRA1.bn.weight\",\n      \"stages.0.0.large_kernel.LoRA1.bn.bias\",\n      \"stages.0.0.large_kernel.LoRA2.bn.weight\",\n      \"stages.0.0.large_kernel.LoRA2.bn.bias\",\n      \"stages.0.0.large_kernel.small_conv.bn.weight\",\n      \"stages.0.0.large_kernel.small_conv.bn.bias\",\n      \"stages.0.0.norm.weight\",\n      \"stages.0.0.norm.bias\",\n      \"stages.0.0.pwconv1.bias\",\n      \"stages.0.0.pwconv2.bias\",\n      \"stages.0.1.gamma\",\n      \"stages.0.1.large_kernel.LoRA1.bn.weight\",\n      \"stages.0.1.large_kernel.LoRA1.bn.bias\",\n      \"stages.0.1.large_kernel.LoRA2.bn.weight\",\n      \"stages.0.1.large_kernel.LoRA2.bn.bias\",\n      \"stages.0.1.large_kernel.small_conv.bn.weight\",\n      \"stages.0.1.large_kernel.small_conv.bn.bias\",\n      \"stages.0.1.norm.weight\",\n      \"stages.0.1.norm.bias\",\n      \"stages.0.1.pwconv1.bias\",\n      \"stages.0.1.pwconv2.bias\",\n      \"stages.0.2.gamma\",\n      \"stages.0.2.large_kernel.LoRA1.bn.weight\",\n      \"stages.0.2.large_kernel.LoRA1.bn.bias\",\n      \"stages.0.2.large_kernel.LoRA2.bn.weight\",\n      \"stages.0.2.large_kernel.LoRA2.bn.bias\",\n      \"stages.0.2.large_kernel.small_conv.bn.weight\",\n      \"stages.0.2.large_kernel.small_conv.bn.bias\",\n      \"stages.0.2.norm.weight\",\n      \"stages.0.2.norm.bias\",\n      \"stages.0.2.pwconv1.bias\",\n      \"stages.0.2.pwconv2.bias\",\n      \"stages.1.0.gamma\",\n      \"stages.1.0.large_kernel.LoRA1.bn.weight\",\n      \"stages.1.0.large_kernel.LoRA1.bn.bias\",\n      \"stages.1.0.large_kernel.LoRA2.bn.weight\",\n      \"stages.1.0.large_kernel.LoRA2.bn.bias\",\n      \"stages.1.0.large_kernel.small_conv.bn.weight\",\n      \"stages.1.0.large_kernel.small_conv.bn.bias\",\n      \"stages.1.0.norm.weight\",\n      \"stages.1.0.norm.bias\",\n      \"stages.1.0.pwconv1.bias\",\n      \"stages.1.0.pwconv2.bias\",\n      \"stages.1.1.gamma\",\n      \"stages.1.1.large_kernel.LoRA1.bn.weight\",\n      \"stages.1.1.large_kernel.LoRA1.bn.bias\",\n      \"stages.1.1.large_kernel.LoRA2.bn.weight\",\n      \"stages.1.1.large_kernel.LoRA2.bn.bias\",\n      \"stages.1.1.large_kernel.small_conv.bn.weight\",\n      \"stages.1.1.large_kernel.small_conv.bn.bias\",\n      \"stages.1.1.norm.weight\",\n      \"stages.1.1.norm.bias\",\n      \"stages.1.1.pwconv1.bias\",\n      \"stages.1.1.pwconv2.bias\",\n      \"stages.1.2.gamma\",\n      \"stages.1.2.large_kernel.LoRA1.bn.weight\",\n      \"stages.1.2.large_kernel.LoRA1.bn.bias\",\n      \"stages.1.2.large_kernel.LoRA2.bn.weight\",\n      \"stages.1.2.large_kernel.LoRA2.bn.bias\",\n      \"stages.1.2.large_kernel.small_conv.bn.weight\",\n      \"stages.1.2.large_kernel.small_conv.bn.bias\",\n      \"stages.1.2.norm.weight\",\n      \"stages.1.2.norm.bias\",\n      \"stages.1.2.pwconv1.bias\",\n      \"stages.1.2.pwconv2.bias\",\n      \"stages.2.0.gamma\",\n      \"stages.2.0.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.0.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.0.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.0.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.0.large_kernel.small_conv.bn.weight\",\n      \"stages.2.0.large_kernel.small_conv.bn.bias\",\n      \"stages.2.0.norm.weight\",\n      \"stages.2.0.norm.bias\",\n      \"stages.2.0.pwconv1.bias\",\n      \"stages.2.0.pwconv2.bias\",\n      \"stages.2.1.gamma\",\n      \"stages.2.1.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.1.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.1.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.1.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.1.large_kernel.small_conv.bn.weight\",\n      \"stages.2.1.large_kernel.small_conv.bn.bias\",\n      \"stages.2.1.norm.weight\",\n      \"stages.2.1.norm.bias\",\n      \"stages.2.1.pwconv1.bias\",\n      \"stages.2.1.pwconv2.bias\",\n      \"stages.2.2.gamma\",\n      \"stages.2.2.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.2.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.2.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.2.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.2.large_kernel.small_conv.bn.weight\",\n      \"stages.2.2.large_kernel.small_conv.bn.bias\",\n      \"stages.2.2.norm.weight\",\n      \"stages.2.2.norm.bias\",\n      \"stages.2.2.pwconv1.bias\",\n      \"stages.2.2.pwconv2.bias\",\n      \"stages.2.3.gamma\",\n      \"stages.2.3.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.3.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.3.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.3.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.3.large_kernel.small_conv.bn.weight\",\n      \"stages.2.3.large_kernel.small_conv.bn.bias\",\n      \"stages.2.3.norm.weight\",\n      \"stages.2.3.norm.bias\",\n      \"stages.2.3.pwconv1.bias\",\n      \"stages.2.3.pwconv2.bias\",\n      \"stages.2.4.gamma\",\n      \"stages.2.4.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.4.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.4.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.4.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.4.large_kernel.small_conv.bn.weight\",\n      \"stages.2.4.large_kernel.small_conv.bn.bias\",\n      \"stages.2.4.norm.weight\",\n      \"stages.2.4.norm.bias\",\n      \"stages.2.4.pwconv1.bias\",\n      \"stages.2.4.pwconv2.bias\",\n      \"stages.2.5.gamma\",\n      \"stages.2.5.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.5.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.5.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.5.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.5.large_kernel.small_conv.bn.weight\",\n      \"stages.2.5.large_kernel.small_conv.bn.bias\",\n      \"stages.2.5.norm.weight\",\n      \"stages.2.5.norm.bias\",\n      \"stages.2.5.pwconv1.bias\",\n      \"stages.2.5.pwconv2.bias\",\n      \"stages.2.6.gamma\",\n      \"stages.2.6.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.6.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.6.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.6.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.6.large_kernel.small_conv.bn.weight\",\n      \"stages.2.6.large_kernel.small_conv.bn.bias\",\n      \"stages.2.6.norm.weight\",\n      \"stages.2.6.norm.bias\",\n      \"stages.2.6.pwconv1.bias\",\n      \"stages.2.6.pwconv2.bias\",\n      \"stages.2.7.gamma\",\n      \"stages.2.7.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.7.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.7.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.7.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.7.large_kernel.small_conv.bn.weight\",\n      \"stages.2.7.large_kernel.small_conv.bn.bias\",\n      \"stages.2.7.norm.weight\",\n      \"stages.2.7.norm.bias\",\n      \"stages.2.7.pwconv1.bias\",\n      \"stages.2.7.pwconv2.bias\",\n      \"stages.2.8.gamma\",\n      \"stages.2.8.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.8.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.8.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.8.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.8.large_kernel.small_conv.bn.weight\",\n      \"stages.2.8.large_kernel.small_conv.bn.bias\",\n      \"stages.2.8.norm.weight\",\n      \"stages.2.8.norm.bias\",\n      \"stages.2.8.pwconv1.bias\",\n      \"stages.2.8.pwconv2.bias\",\n      \"stages.2.9.gamma\",\n      \"stages.2.9.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.9.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.9.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.9.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.9.large_kernel.small_conv.bn.weight\",\n      \"stages.2.9.large_kernel.small_conv.bn.bias\",\n      \"stages.2.9.norm.weight\",\n      \"stages.2.9.norm.bias\",\n      \"stages.2.9.pwconv1.bias\",\n      \"stages.2.9.pwconv2.bias\",\n      \"stages.2.10.gamma\",\n      \"stages.2.10.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.10.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.10.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.10.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.10.large_kernel.small_conv.bn.weight\",\n      \"stages.2.10.large_kernel.small_conv.bn.bias\",\n      \"stages.2.10.norm.weight\",\n      \"stages.2.10.norm.bias\",\n      \"stages.2.10.pwconv1.bias\",\n      \"stages.2.10.pwconv2.bias\",\n      \"stages.2.11.gamma\",\n      \"stages.2.11.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.11.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.11.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.11.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.11.large_kernel.small_conv.bn.weight\",\n      \"stages.2.11.large_kernel.small_conv.bn.bias\",\n      \"stages.2.11.norm.weight\",\n      \"stages.2.11.norm.bias\",\n      \"stages.2.11.pwconv1.bias\",\n      \"stages.2.11.pwconv2.bias\",\n      \"stages.2.12.gamma\",\n      \"stages.2.12.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.12.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.12.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.12.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.12.large_kernel.small_conv.bn.weight\",\n      \"stages.2.12.large_kernel.small_conv.bn.bias\",\n      \"stages.2.12.norm.weight\",\n      \"stages.2.12.norm.bias\",\n      \"stages.2.12.pwconv1.bias\",\n      \"stages.2.12.pwconv2.bias\",\n      \"stages.2.13.gamma\",\n      \"stages.2.13.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.13.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.13.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.13.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.13.large_kernel.small_conv.bn.weight\",\n      \"stages.2.13.large_kernel.small_conv.bn.bias\",\n      \"stages.2.13.norm.weight\",\n      \"stages.2.13.norm.bias\",\n      \"stages.2.13.pwconv1.bias\",\n      \"stages.2.13.pwconv2.bias\",\n      \"stages.2.14.gamma\",\n      \"stages.2.14.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.14.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.14.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.14.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.14.large_kernel.small_conv.bn.weight\",\n      \"stages.2.14.large_kernel.small_conv.bn.bias\",\n      \"stages.2.14.norm.weight\",\n      \"stages.2.14.norm.bias\",\n      \"stages.2.14.pwconv1.bias\",\n      \"stages.2.14.pwconv2.bias\",\n      \"stages.2.15.gamma\",\n      \"stages.2.15.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.15.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.15.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.15.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.15.large_kernel.small_conv.bn.weight\",\n      \"stages.2.15.large_kernel.small_conv.bn.bias\",\n      \"stages.2.15.norm.weight\",\n      \"stages.2.15.norm.bias\",\n      \"stages.2.15.pwconv1.bias\",\n      \"stages.2.15.pwconv2.bias\",\n      \"stages.2.16.gamma\",\n      \"stages.2.16.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.16.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.16.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.16.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.16.large_kernel.small_conv.bn.weight\",\n      \"stages.2.16.large_kernel.small_conv.bn.bias\",\n      \"stages.2.16.norm.weight\",\n      \"stages.2.16.norm.bias\",\n      \"stages.2.16.pwconv1.bias\",\n      \"stages.2.16.pwconv2.bias\",\n      \"stages.2.17.gamma\",\n      \"stages.2.17.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.17.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.17.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.17.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.17.large_kernel.small_conv.bn.weight\",\n      \"stages.2.17.large_kernel.small_conv.bn.bias\",\n      \"stages.2.17.norm.weight\",\n      \"stages.2.17.norm.bias\",\n      \"stages.2.17.pwconv1.bias\",\n      \"stages.2.17.pwconv2.bias\",\n      \"stages.2.18.gamma\",\n      \"stages.2.18.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.18.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.18.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.18.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.18.large_kernel.small_conv.bn.weight\",\n      \"stages.2.18.large_kernel.small_conv.bn.bias\",\n      \"stages.2.18.norm.weight\",\n      \"stages.2.18.norm.bias\",\n      \"stages.2.18.pwconv1.bias\",\n      \"stages.2.18.pwconv2.bias\",\n      \"stages.2.19.gamma\",\n      \"stages.2.19.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.19.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.19.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.19.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.19.large_kernel.small_conv.bn.weight\",\n      \"stages.2.19.large_kernel.small_conv.bn.bias\",\n      \"stages.2.19.norm.weight\",\n      \"stages.2.19.norm.bias\",\n      \"stages.2.19.pwconv1.bias\",\n      \"stages.2.19.pwconv2.bias\",\n      \"stages.2.20.gamma\",\n      \"stages.2.20.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.20.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.20.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.20.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.20.large_kernel.small_conv.bn.weight\",\n      \"stages.2.20.large_kernel.small_conv.bn.bias\",\n      \"stages.2.20.norm.weight\",\n      \"stages.2.20.norm.bias\",\n      \"stages.2.20.pwconv1.bias\",\n      \"stages.2.20.pwconv2.bias\",\n      \"stages.2.21.gamma\",\n      \"stages.2.21.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.21.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.21.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.21.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.21.large_kernel.small_conv.bn.weight\",\n      \"stages.2.21.large_kernel.small_conv.bn.bias\",\n      \"stages.2.21.norm.weight\",\n      \"stages.2.21.norm.bias\",\n      \"stages.2.21.pwconv1.bias\",\n      \"stages.2.21.pwconv2.bias\",\n      \"stages.2.22.gamma\",\n      \"stages.2.22.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.22.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.22.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.22.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.22.large_kernel.small_conv.bn.weight\",\n      \"stages.2.22.large_kernel.small_conv.bn.bias\",\n      \"stages.2.22.norm.weight\",\n      \"stages.2.22.norm.bias\",\n      \"stages.2.22.pwconv1.bias\",\n      \"stages.2.22.pwconv2.bias\",\n      \"stages.2.23.gamma\",\n      \"stages.2.23.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.23.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.23.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.23.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.23.large_kernel.small_conv.bn.weight\",\n      \"stages.2.23.large_kernel.small_conv.bn.bias\",\n      \"stages.2.23.norm.weight\",\n      \"stages.2.23.norm.bias\",\n      \"stages.2.23.pwconv1.bias\",\n      \"stages.2.23.pwconv2.bias\",\n      \"stages.2.24.gamma\",\n      \"stages.2.24.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.24.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.24.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.24.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.24.large_kernel.small_conv.bn.weight\",\n      \"stages.2.24.large_kernel.small_conv.bn.bias\",\n      \"stages.2.24.norm.weight\",\n      \"stages.2.24.norm.bias\",\n      \"stages.2.24.pwconv1.bias\",\n      \"stages.2.24.pwconv2.bias\",\n      \"stages.2.25.gamma\",\n      \"stages.2.25.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.25.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.25.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.25.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.25.large_kernel.small_conv.bn.weight\",\n      \"stages.2.25.large_kernel.small_conv.bn.bias\",\n      \"stages.2.25.norm.weight\",\n      \"stages.2.25.norm.bias\",\n      \"stages.2.25.pwconv1.bias\",\n      \"stages.2.25.pwconv2.bias\",\n      \"stages.2.26.gamma\",\n      \"stages.2.26.large_kernel.LoRA1.bn.weight\",\n      \"stages.2.26.large_kernel.LoRA1.bn.bias\",\n      \"stages.2.26.large_kernel.LoRA2.bn.weight\",\n      \"stages.2.26.large_kernel.LoRA2.bn.bias\",\n      \"stages.2.26.large_kernel.small_conv.bn.weight\",\n      \"stages.2.26.large_kernel.small_conv.bn.bias\",\n      \"stages.2.26.norm.weight\",\n      \"stages.2.26.norm.bias\",\n      \"stages.2.26.pwconv1.bias\",\n      \"stages.2.26.pwconv2.bias\",\n      \"stages.3.0.gamma\",\n      \"stages.3.0.large_kernel.LoRA1.bn.weight\",\n      \"stages.3.0.large_kernel.LoRA1.bn.bias\",\n      \"stages.3.0.large_kernel.LoRA2.bn.weight\",\n      \"stages.3.0.large_kernel.LoRA2.bn.bias\",\n      \"stages.3.0.large_kernel.small_conv.bn.weight\",\n      \"stages.3.0.large_kernel.small_conv.bn.bias\",\n      \"stages.3.0.norm.weight\",\n      \"stages.3.0.norm.bias\",\n      \"stages.3.0.pwconv1.bias\",\n      \"stages.3.0.pwconv2.bias\",\n      \"stages.3.1.gamma\",\n      \"stages.3.1.large_kernel.LoRA1.bn.weight\",\n      \"stages.3.1.large_kernel.LoRA1.bn.bias\",\n      \"stages.3.1.large_kernel.LoRA2.bn.weight\",\n      \"stages.3.1.large_kernel.LoRA2.bn.bias\",\n      \"stages.3.1.large_kernel.small_conv.bn.weight\",\n      \"stages.3.1.large_kernel.small_conv.bn.bias\",\n      \"stages.3.1.norm.weight\",\n      \"stages.3.1.norm.bias\",\n      \"stages.3.1.pwconv1.bias\",\n      \"stages.3.1.pwconv2.bias\",\n      \"stages.3.2.gamma\",\n      \"stages.3.2.large_kernel.LoRA1.bn.weight\",\n      \"stages.3.2.large_kernel.LoRA1.bn.bias\",\n      \"stages.3.2.large_kernel.LoRA2.bn.weight\",\n      \"stages.3.2.large_kernel.LoRA2.bn.bias\",\n      \"stages.3.2.large_kernel.small_conv.bn.weight\",\n      \"stages.3.2.large_kernel.small_conv.bn.bias\",\n      \"stages.3.2.norm.weight\",\n      \"stages.3.2.norm.bias\",\n      \"stages.3.2.pwconv1.bias\",\n      \"stages.3.2.pwconv2.bias\",\n      \"norm.weight\",\n      \"norm.bias\",\n      \"head.bias\"\n    ],\n    \"lr_scale\": 1.0\n  }\n}\nUse Cosine LR scheduler\nSet warmup steps = 15620\nSet warmup steps = 0\nMax WD = 0.0500000, Min WD = 0.0500000\ncriterion = SoftTargetCrossEntropy()\nAuto resume checkpoint: /kaggle/input/slakbase-weights/checkpoint-best-ema_slakbaseema.pth\nResume checkpoint /kaggle/input/slakbase-weights/checkpoint-best-ema_slakbaseema.pth\nWith optim & sched!\ndownsample_layers.0.0.weight density is 1.0\ndownsample_layers.0.0.bias density is 1.0\ndownsample_layers.0.1.weight density is 1.0\ndownsample_layers.0.1.bias density is 1.0\ndownsample_layers.1.0.weight density is 1.0\ndownsample_layers.1.0.bias density is 1.0\ndownsample_layers.1.1.weight density is 1.0\ndownsample_layers.1.1.bias density is 1.0\ndownsample_layers.2.0.weight density is 1.0\ndownsample_layers.2.0.bias density is 1.0\ndownsample_layers.2.1.weight density is 1.0\ndownsample_layers.2.1.bias density is 1.0\ndownsample_layers.3.0.weight density is 1.0\ndownsample_layers.3.0.bias density is 1.0\ndownsample_layers.3.1.weight density is 1.0\ndownsample_layers.3.1.bias density is 1.0\nstages.0.0.gamma density is 1.0\nstages.0.0.large_kernel.LoRA1.conv.weight density is 0.18410111032364754\nstages.0.0.large_kernel.LoRA1.bn.weight density is 1.0\nstages.0.0.large_kernel.LoRA1.bn.bias density is 1.0\nstages.0.0.large_kernel.LoRA2.conv.weight density is 0.207677769903142\nstages.0.0.large_kernel.LoRA2.bn.weight density is 1.0\nstages.0.0.large_kernel.LoRA2.bn.bias density is 1.0\nstages.0.0.large_kernel.small_conv.conv.weight density is 0.8785542168674699\nstages.0.0.large_kernel.small_conv.bn.weight density is 1.0\nstages.0.0.large_kernel.small_conv.bn.bias density is 1.0\nstages.0.0.norm.weight density is 1.0\nstages.0.0.norm.bias density is 1.0\nstages.0.0.pwconv1.weight density is 0.5248221802874147\nstages.0.0.pwconv1.bias density is 1.0\nstages.0.0.pwconv2.weight density is 0.514833430105966\nstages.0.0.pwconv2.bias density is 1.0\nstages.0.1.gamma density is 1.0\nstages.0.1.large_kernel.LoRA1.conv.weight density is 0.19355067328136075\nstages.0.1.large_kernel.LoRA1.bn.weight density is 1.0\nstages.0.1.large_kernel.LoRA1.bn.bias density is 1.0\nstages.0.1.large_kernel.LoRA2.conv.weight density is 0.17850224427120245\nstages.0.1.large_kernel.LoRA2.bn.weight density is 1.0\nstages.0.1.large_kernel.LoRA2.bn.bias density is 1.0\nstages.0.1.large_kernel.small_conv.conv.weight density is 0.7727710843373494\nstages.0.1.large_kernel.small_conv.bn.weight density is 1.0\nstages.0.1.large_kernel.small_conv.bn.bias density is 1.0\nstages.0.1.norm.weight density is 1.0\nstages.0.1.norm.bias density is 1.0\nstages.0.1.pwconv1.weight density is 0.5233070837567136\nstages.0.1.pwconv1.bias density is 1.0\nstages.0.1.pwconv2.weight density is 0.5339671940775149\nstages.0.1.pwconv2.bias density is 1.0\nstages.0.2.gamma density is 1.0\nstages.0.2.large_kernel.LoRA1.conv.weight density is 0.19352704937396645\nstages.0.2.large_kernel.LoRA1.bn.weight density is 1.0\nstages.0.2.large_kernel.LoRA1.bn.bias density is 1.0\nstages.0.2.large_kernel.LoRA2.conv.weight density is 0.19492085991022914\nstages.0.2.large_kernel.LoRA2.bn.weight density is 1.0\nstages.0.2.large_kernel.LoRA2.bn.bias density is 1.0\nstages.0.2.large_kernel.small_conv.conv.weight density is 0.7660240963855421\nstages.0.2.large_kernel.small_conv.bn.weight density is 1.0\nstages.0.2.large_kernel.small_conv.bn.bias density is 1.0\nstages.0.2.norm.weight density is 1.0\nstages.0.2.norm.bias density is 1.0\nstages.0.2.pwconv1.weight density is 0.5330055160400639\nstages.0.2.pwconv1.bias density is 1.0\nstages.0.2.pwconv2.weight density is 0.5210571200464509\nstages.0.2.pwconv2.bias density is 1.0\nstages.1.0.gamma density is 1.0\nstages.1.0.large_kernel.LoRA1.conv.weight density is 0.11449471354806982\nstages.1.0.large_kernel.LoRA1.bn.weight density is 1.0\nstages.1.0.large_kernel.LoRA1.bn.bias density is 1.0\nstages.1.0.large_kernel.LoRA2.conv.weight density is 0.12024834029997541\nstages.1.0.large_kernel.LoRA2.bn.weight density is 1.0\nstages.1.0.large_kernel.LoRA2.bn.bias density is 1.0\nstages.1.0.large_kernel.small_conv.conv.weight density is 0.8501204819277108\nstages.1.0.large_kernel.small_conv.bn.weight density is 1.0\nstages.1.0.large_kernel.small_conv.bn.bias density is 1.0\nstages.1.0.norm.weight density is 1.0\nstages.1.0.norm.bias density is 1.0\nstages.1.0.pwconv1.weight density is 0.6971553382203513\nstages.1.0.pwconv1.bias density is 1.0\nstages.1.0.pwconv2.weight density is 0.6991558099869357\nstages.1.0.pwconv2.bias density is 1.0\nstages.1.1.gamma density is 1.0\nstages.1.1.large_kernel.LoRA1.conv.weight density is 0.12523973444799608\nstages.1.1.large_kernel.LoRA1.bn.weight density is 1.0\nstages.1.1.large_kernel.LoRA1.bn.bias density is 1.0\nstages.1.1.large_kernel.LoRA2.conv.weight density is 0.12916154413572659\nstages.1.1.large_kernel.LoRA2.bn.weight density is 1.0\nstages.1.1.large_kernel.LoRA2.bn.bias density is 1.0\nstages.1.1.large_kernel.small_conv.conv.weight density is 0.8998795180722892\nstages.1.1.large_kernel.small_conv.bn.weight density is 1.0\nstages.1.1.large_kernel.small_conv.bn.bias density is 1.0\nstages.1.1.norm.weight density is 1.0\nstages.1.1.norm.bias density is 1.0\nstages.1.1.pwconv1.weight density is 0.6893916025547975\nstages.1.1.pwconv1.bias density is 1.0\nstages.1.1.pwconv2.weight density is 0.6972528668892437\nstages.1.1.pwconv2.bias density is 1.0\nstages.1.2.gamma density is 1.0\nstages.1.2.large_kernel.LoRA1.conv.weight density is 0.12249815588886157\nstages.1.2.large_kernel.LoRA1.bn.weight density is 1.0\nstages.1.2.large_kernel.LoRA1.bn.bias density is 1.0\nstages.1.2.large_kernel.LoRA2.conv.weight density is 0.12387509220555692\nstages.1.2.large_kernel.LoRA2.bn.weight density is 1.0\nstages.1.2.large_kernel.LoRA2.bn.bias density is 1.0\nstages.1.2.large_kernel.small_conv.conv.weight density is 0.9306024096385542\nstages.1.2.large_kernel.small_conv.bn.weight density is 1.0\nstages.1.2.large_kernel.small_conv.bn.bias density is 1.0\nstages.1.2.norm.weight density is 1.0\nstages.1.2.norm.bias density is 1.0\nstages.1.2.pwconv1.weight density is 0.700709464363478\nstages.1.2.pwconv1.bias density is 1.0\nstages.1.2.pwconv2.weight density is 0.6826371752068515\nstages.1.2.pwconv2.bias density is 1.0\nstages.2.0.gamma density is 1.0\nstages.2.0.large_kernel.LoRA1.conv.weight density is 0.035411934090545515\nstages.2.0.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.0.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.0.large_kernel.LoRA2.conv.weight density is 0.03597504399296113\nstages.2.0.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.0.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.0.large_kernel.small_conv.conv.weight density is 0.29076691729323306\nstages.2.0.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.0.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.0.norm.weight density is 1.0\nstages.2.0.norm.bias density is 1.0\nstages.2.0.pwconv1.weight density is 0.6334818248629092\nstages.2.0.pwconv1.bias density is 1.0\nstages.2.0.pwconv2.weight density is 0.6481338685058511\nstages.2.0.pwconv2.bias density is 1.0\nstages.2.1.gamma density is 1.0\nstages.2.1.large_kernel.LoRA1.conv.weight density is 0.035277555591105424\nstages.2.1.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.1.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.1.large_kernel.LoRA2.conv.weight density is 0.03598784194528876\nstages.2.1.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.1.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.1.large_kernel.small_conv.conv.weight density is 0.3362406015037594\nstages.2.1.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.1.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.1.norm.weight density is 1.0\nstages.2.1.norm.bias density is 1.0\nstages.2.1.pwconv1.weight density is 0.6273961218836565\nstages.2.1.pwconv1.bias density is 1.0\nstages.2.1.pwconv2.weight density is 0.632554129685115\nstages.2.1.pwconv2.bias density is 1.0\nstages.2.2.gamma density is 1.0\nstages.2.2.large_kernel.LoRA1.conv.weight density is 0.02951847704367301\nstages.2.2.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.2.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.2.large_kernel.LoRA2.conv.weight density is 0.03604543273076308\nstages.2.2.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.2.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.2.large_kernel.small_conv.conv.weight density is 0.3054436090225564\nstages.2.2.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.2.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.2.norm.weight density is 1.0\nstages.2.2.norm.bias density is 1.0\nstages.2.2.pwconv1.weight density is 0.6255249024817683\nstages.2.2.pwconv1.bias density is 1.0\nstages.2.2.pwconv2.weight density is 0.6326044434394257\nstages.2.2.pwconv2.bias density is 1.0\nstages.2.3.gamma density is 1.0\nstages.2.3.large_kernel.LoRA1.conv.weight density is 0.03605183170692689\nstages.2.3.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.3.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.3.large_kernel.LoRA2.conv.weight density is 0.03819548872180451\nstages.2.3.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.3.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.3.large_kernel.small_conv.conv.weight density is 0.3459248120300752\nstages.2.3.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.3.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.3.norm.weight density is 1.0\nstages.2.3.norm.bias density is 1.0\nstages.2.3.pwconv1.weight density is 0.6419153146022952\nstages.2.3.pwconv1.bias density is 1.0\nstages.2.3.pwconv2.weight density is 0.6503793317881169\nstages.2.3.pwconv2.bias density is 1.0\nstages.2.4.gamma density is 1.0\nstages.2.4.large_kernel.LoRA1.conv.weight density is 0.03514317709166533\nstages.2.4.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.4.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.4.large_kernel.LoRA2.conv.weight density is 0.03547592385218365\nstages.2.4.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.4.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.4.large_kernel.small_conv.conv.weight density is 0.334796992481203\nstages.2.4.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.4.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.4.norm.weight density is 1.0\nstages.2.4.norm.bias density is 1.0\nstages.2.4.pwconv1.weight density is 0.6311453445644186\nstages.2.4.pwconv1.bias density is 1.0\nstages.2.4.pwconv2.weight density is 0.6395042116569619\nstages.2.4.pwconv2.bias density is 1.0\nstages.2.5.gamma density is 1.0\nstages.2.5.large_kernel.LoRA1.conv.weight density is 0.03748520236762118\nstages.2.5.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.5.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.5.large_kernel.LoRA2.conv.weight density is 0.035207166853303475\nstages.2.5.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.5.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.5.large_kernel.small_conv.conv.weight density is 0.35254135338345866\nstages.2.5.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.5.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.5.norm.weight density is 1.0\nstages.2.5.norm.bias density is 1.0\nstages.2.5.pwconv1.weight density is 0.6414636214596642\nstages.2.5.pwconv1.bias density is 1.0\nstages.2.5.pwconv2.weight density is 0.6535818870484482\nstages.2.5.pwconv2.bias density is 1.0\nstages.2.6.gamma density is 1.0\nstages.2.6.large_kernel.LoRA1.conv.weight density is 0.03545672692369221\nstages.2.6.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.6.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.6.large_kernel.LoRA2.conv.weight density is 0.035232762757958724\nstages.2.6.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.6.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.6.large_kernel.small_conv.conv.weight density is 0.3313082706766917\nstages.2.6.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.6.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.6.norm.weight density is 1.0\nstages.2.6.norm.bias density is 1.0\nstages.2.6.pwconv1.weight density is 0.6504409520040704\nstages.2.6.pwconv1.bias density is 1.0\nstages.2.6.pwconv2.weight density is 0.6526061394086721\nstages.2.6.pwconv2.bias density is 1.0\nstages.2.7.gamma density is 1.0\nstages.2.7.large_kernel.LoRA1.conv.weight density is 0.03573828187490002\nstages.2.7.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.7.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.7.large_kernel.LoRA2.conv.weight density is 0.034810430331147016\nstages.2.7.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.7.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.7.large_kernel.small_conv.conv.weight density is 0.3302857142857143\nstages.2.7.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.7.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.7.norm.weight density is 1.0\nstages.2.7.norm.bias density is 1.0\nstages.2.7.pwconv1.weight density is 0.6357634688224321\nstages.2.7.pwconv1.bias density is 1.0\nstages.2.7.pwconv2.weight density is 0.6368579343094578\nstages.2.7.pwconv2.bias density is 1.0\nstages.2.8.gamma density is 1.0\nstages.2.8.large_kernel.LoRA1.conv.weight density is 0.032122860342345225\nstages.2.8.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.8.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.8.large_kernel.LoRA2.conv.weight density is 0.032225243960966245\nstages.2.8.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.8.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.8.large_kernel.small_conv.conv.weight density is 0.3281203007518797\nstages.2.8.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.8.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.8.norm.weight density is 1.0\nstages.2.8.norm.bias density is 1.0\nstages.2.8.pwconv1.weight density is 0.6241313810842897\nstages.2.8.pwconv1.bias density is 1.0\nstages.2.8.pwconv2.weight density is 0.6316326530612245\nstages.2.8.pwconv2.bias density is 1.0\nstages.2.9.gamma density is 1.0\nstages.2.9.large_kernel.LoRA1.conv.weight density is 0.03522636378179491\nstages.2.9.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.9.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.9.large_kernel.LoRA2.conv.weight density is 0.03756199008158695\nstages.2.9.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.9.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.9.large_kernel.small_conv.conv.weight density is 0.3098345864661654\nstages.2.9.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.9.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.9.norm.weight density is 1.0\nstages.2.9.norm.bias density is 1.0\nstages.2.9.pwconv1.weight density is 0.645635705805868\nstages.2.9.pwconv1.bias density is 1.0\nstages.2.9.pwconv2.weight density is 0.6503810277573633\nstages.2.9.pwconv2.bias density is 1.0\nstages.2.10.gamma density is 1.0\nstages.2.10.large_kernel.LoRA1.conv.weight density is 0.035059990401535755\nstages.2.10.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.10.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.10.large_kernel.LoRA2.conv.weight density is 0.037657974724044155\nstages.2.10.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.10.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.10.large_kernel.small_conv.conv.weight density is 0.31434586466165415\nstages.2.10.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.10.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.10.norm.weight density is 1.0\nstages.2.10.norm.bias density is 1.0\nstages.2.10.pwconv1.weight density is 0.6520442082650234\nstages.2.10.pwconv1.bias density is 1.0\nstages.2.10.pwconv2.weight density is 0.6578591214879304\nstages.2.10.pwconv2.bias density is 1.0\nstages.2.11.gamma density is 1.0\nstages.2.11.large_kernel.LoRA1.conv.weight density is 0.032430011198208285\nstages.2.11.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.11.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.11.large_kernel.LoRA2.conv.weight density is 0.032122860342345225\nstages.2.11.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.11.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.11.large_kernel.small_conv.conv.weight density is 0.31434586466165415\nstages.2.11.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.11.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.11.norm.weight density is 1.0\nstages.2.11.norm.bias density is 1.0\nstages.2.11.pwconv1.weight density is 0.641690881338685\nstages.2.11.pwconv1.bias density is 1.0\nstages.2.11.pwconv2.weight density is 0.6550816891853695\nstages.2.11.pwconv2.bias density is 1.0\nstages.2.12.gamma density is 1.0\nstages.2.12.large_kernel.LoRA1.conv.weight density is 0.03376099824028155\nstages.2.12.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.12.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.12.large_kernel.LoRA2.conv.weight density is 0.03386338185890258\nstages.2.12.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.12.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.12.large_kernel.small_conv.conv.weight density is 0.35157894736842105\nstages.2.12.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.12.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.12.norm.weight density is 1.0\nstages.2.12.norm.bias density is 1.0\nstages.2.12.pwconv1.weight density is 0.650931087116287\nstages.2.12.pwconv1.bias density is 1.0\nstages.2.12.pwconv2.weight density is 0.6531488495675278\nstages.2.12.pwconv2.bias density is 1.0\nstages.2.13.gamma density is 1.0\nstages.2.13.large_kernel.LoRA1.conv.weight density is 0.03694768836986082\nstages.2.13.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.13.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.13.large_kernel.LoRA2.conv.weight density is 0.03593665013597824\nstages.2.13.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.13.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.13.large_kernel.small_conv.conv.weight density is 0.33371428571428574\nstages.2.13.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.13.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.13.norm.weight density is 1.0\nstages.2.13.norm.bias density is 1.0\nstages.2.13.pwconv1.weight density is 0.6556385324212788\nstages.2.13.pwconv1.bias density is 1.0\nstages.2.13.pwconv2.weight density is 0.6619927638645486\nstages.2.13.pwconv2.bias density is 1.0\nstages.2.14.gamma density is 1.0\nstages.2.14.large_kernel.LoRA1.conv.weight density is 0.03542473204287314\nstages.2.14.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.14.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.14.large_kernel.LoRA2.conv.weight density is 0.03234042553191489\nstages.2.14.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.14.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.14.large_kernel.small_conv.conv.weight density is 0.35849624060150376\nstages.2.14.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.14.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.14.norm.weight density is 1.0\nstages.2.14.norm.bias density is 1.0\nstages.2.14.pwconv1.weight density is 0.6335751031714625\nstages.2.14.pwconv1.bias density is 1.0\nstages.2.14.pwconv2.weight density is 0.6394572898411442\nstages.2.14.pwconv2.bias density is 1.0\nstages.2.15.gamma density is 1.0\nstages.2.15.large_kernel.LoRA1.conv.weight density is 0.03405535114381699\nstages.2.15.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.15.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.15.large_kernel.LoRA2.conv.weight density is 0.03500239961606143\nstages.2.15.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.15.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.15.large_kernel.small_conv.conv.weight density is 0.30851127819548874\nstages.2.15.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.15.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.15.norm.weight density is 1.0\nstages.2.15.norm.bias density is 1.0\nstages.2.15.pwconv1.weight density is 0.6788043416812708\nstages.2.15.pwconv1.bias density is 1.0\nstages.2.15.pwconv2.weight density is 0.6822330261744587\nstages.2.15.pwconv2.bias density is 1.0\nstages.2.16.gamma density is 1.0\nstages.2.16.large_kernel.LoRA1.conv.weight density is 0.03077907534794433\nstages.2.16.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.16.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.16.large_kernel.LoRA2.conv.weight density is 0.035911054231322986\nstages.2.16.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.16.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.16.large_kernel.small_conv.conv.weight density is 0.31657142857142856\nstages.2.16.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.16.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.16.norm.weight density is 1.0\nstages.2.16.norm.bias density is 1.0\nstages.2.16.pwconv1.weight density is 0.6714381819209678\nstages.2.16.pwconv1.bias density is 1.0\nstages.2.16.pwconv2.weight density is 0.683313358584431\nstages.2.16.pwconv2.bias density is 1.0\nstages.2.17.gamma density is 1.0\nstages.2.17.large_kernel.LoRA1.conv.weight density is 0.03768357062869941\nstages.2.17.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.17.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.17.large_kernel.LoRA2.conv.weight density is 0.0354055351143817\nstages.2.17.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.17.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.17.large_kernel.small_conv.conv.weight density is 0.3330526315789474\nstages.2.17.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.17.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.17.norm.weight density is 1.0\nstages.2.17.norm.bias density is 1.0\nstages.2.17.pwconv1.weight density is 0.6596223641811295\nstages.2.17.pwconv1.bias density is 1.0\nstages.2.17.pwconv2.weight density is 0.6694996890723048\nstages.2.17.pwconv2.bias density is 1.0\nstages.2.18.gamma density is 1.0\nstages.2.18.large_kernel.LoRA1.conv.weight density is 0.0316941289393697\nstages.2.18.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.18.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.18.large_kernel.LoRA2.conv.weight density is 0.03710766277395617\nstages.2.18.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.18.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.18.large_kernel.small_conv.conv.weight density is 0.33690225563909776\nstages.2.18.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.18.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.18.norm.weight density is 1.0\nstages.2.18.norm.bias density is 1.0\nstages.2.18.pwconv1.weight density is 0.6614297020747357\nstages.2.18.pwconv1.bias density is 1.0\nstages.2.18.pwconv2.weight density is 0.6666182373226299\nstages.2.18.pwconv2.bias density is 1.0\nstages.2.19.gamma density is 1.0\nstages.2.19.large_kernel.LoRA1.conv.weight density is 0.03258358662613982\nstages.2.19.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.19.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.19.large_kernel.LoRA2.conv.weight density is 0.033882578787394015\nstages.2.19.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.19.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.19.large_kernel.small_conv.conv.weight density is 0.33377443609022556\nstages.2.19.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.19.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.19.norm.weight density is 1.0\nstages.2.19.norm.bias density is 1.0\nstages.2.19.pwconv1.weight density is 0.6831680705523206\nstages.2.19.pwconv1.bias density is 1.0\nstages.2.19.pwconv2.weight density is 0.694992368138391\nstages.2.19.pwconv2.bias density is 1.0\nstages.2.20.gamma density is 1.0\nstages.2.20.large_kernel.LoRA1.conv.weight density is 0.036288593824988\nstages.2.20.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.20.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.20.large_kernel.LoRA2.conv.weight density is 0.03549512078067509\nstages.2.20.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.20.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.20.large_kernel.small_conv.conv.weight density is 0.35795488721804514\nstages.2.20.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.20.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.20.norm.weight density is 1.0\nstages.2.20.norm.bias density is 1.0\nstages.2.20.pwconv1.weight density is 0.6671485103736785\nstages.2.20.pwconv1.bias density is 1.0\nstages.2.20.pwconv2.weight density is 0.6746531742891062\nstages.2.20.pwconv2.bias density is 1.0\nstages.2.21.gamma density is 1.0\nstages.2.21.large_kernel.LoRA1.conv.weight density is 0.03100303951367781\nstages.2.21.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.21.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.21.large_kernel.LoRA2.conv.weight density is 0.034253719404895216\nstages.2.21.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.21.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.21.large_kernel.small_conv.conv.weight density is 0.3462857142857143\nstages.2.21.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.21.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.21.norm.weight density is 1.0\nstages.2.21.norm.bias density is 1.0\nstages.2.21.pwconv1.weight density is 0.6821991067895302\nstages.2.21.pwconv1.bias density is 1.0\nstages.2.21.pwconv2.weight density is 0.7000893210469783\nstages.2.21.pwconv2.bias density is 1.0\nstages.2.22.gamma density is 1.0\nstages.2.22.large_kernel.LoRA1.conv.weight density is 0.029761638137897936\nstages.2.22.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.22.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.22.large_kernel.LoRA2.conv.weight density is 0.03180931051031835\nstages.2.22.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.22.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.22.large_kernel.small_conv.conv.weight density is 0.31109774436090226\nstages.2.22.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.22.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.22.norm.weight density is 1.0\nstages.2.22.norm.bias density is 1.0\nstages.2.22.pwconv1.weight density is 0.6319000508790774\nstages.2.22.pwconv1.bias density is 1.0\nstages.2.22.pwconv2.weight density is 0.6401486799706032\nstages.2.22.pwconv2.bias density is 1.0\nstages.2.23.gamma density is 1.0\nstages.2.23.large_kernel.LoRA1.conv.weight density is 0.03081746920492721\nstages.2.23.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.23.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.23.large_kernel.LoRA2.conv.weight density is 0.034266517357222845\nstages.2.23.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.23.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.23.large_kernel.small_conv.conv.weight density is 0.3198796992481203\nstages.2.23.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.23.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.23.norm.weight density is 1.0\nstages.2.23.norm.bias density is 1.0\nstages.2.23.pwconv1.weight density is 0.6799576007688394\nstages.2.23.pwconv1.bias density is 1.0\nstages.2.23.pwconv2.weight density is 0.6961241449488382\nstages.2.23.pwconv2.bias density is 1.0\nstages.2.24.gamma density is 1.0\nstages.2.24.large_kernel.LoRA1.conv.weight density is 0.03807390817469205\nstages.2.24.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.24.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.24.large_kernel.LoRA2.conv.weight density is 0.04019196928491441\nstages.2.24.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.24.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.24.large_kernel.small_conv.conv.weight density is 0.3544661654135338\nstages.2.24.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.24.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.24.norm.weight density is 1.0\nstages.2.24.norm.bias density is 1.0\nstages.2.24.pwconv1.weight density is 0.6561501498106168\nstages.2.24.pwconv1.bias density is 1.0\nstages.2.24.pwconv2.weight density is 0.663977047882865\nstages.2.24.pwconv2.bias density is 1.0\nstages.2.25.gamma density is 1.0\nstages.2.25.large_kernel.LoRA1.conv.weight density is 0.03766437370020797\nstages.2.25.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.25.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.25.large_kernel.LoRA2.conv.weight density is 0.03296752519596864\nstages.2.25.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.25.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.25.large_kernel.small_conv.conv.weight density is 0.33870676691729323\nstages.2.25.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.25.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.25.norm.weight density is 1.0\nstages.2.25.norm.bias density is 1.0\nstages.2.25.pwconv1.weight density is 0.6722827746056872\nstages.2.25.pwconv1.bias density is 1.0\nstages.2.25.pwconv2.weight density is 0.684136468992029\nstages.2.25.pwconv2.bias density is 1.0\nstages.2.26.gamma density is 1.0\nstages.2.26.large_kernel.LoRA1.conv.weight density is 0.034048952167653176\nstages.2.26.large_kernel.LoRA1.bn.weight density is 1.0\nstages.2.26.large_kernel.LoRA1.bn.bias density is 1.0\nstages.2.26.large_kernel.LoRA2.conv.weight density is 0.037952327627579586\nstages.2.26.large_kernel.LoRA2.bn.weight density is 1.0\nstages.2.26.large_kernel.LoRA2.bn.bias density is 1.0\nstages.2.26.large_kernel.small_conv.conv.weight density is 0.3297443609022556\nstages.2.26.large_kernel.small_conv.bn.weight density is 1.0\nstages.2.26.large_kernel.small_conv.bn.bias density is 1.0\nstages.2.26.norm.weight density is 1.0\nstages.2.26.norm.bias density is 1.0\nstages.2.26.pwconv1.weight density is 0.7043620329018033\nstages.2.26.pwconv1.bias density is 1.0\nstages.2.26.pwconv2.weight density is 0.7045825089038386\nstages.2.26.pwconv2.bias density is 1.0\nstages.3.0.gamma density is 1.0\nstages.3.0.large_kernel.LoRA1.conv.weight density is 0.009697740276252673\nstages.3.0.large_kernel.LoRA1.bn.weight density is 1.0\nstages.3.0.large_kernel.LoRA1.bn.bias density is 1.0\nstages.3.0.large_kernel.LoRA2.conv.weight density is 0.013095994914176732\nstages.3.0.large_kernel.LoRA2.bn.weight density is 1.0\nstages.3.0.large_kernel.LoRA2.bn.bias density is 1.0\nstages.3.0.large_kernel.small_conv.conv.weight density is 0.0312847483095417\nstages.3.0.large_kernel.small_conv.bn.weight density is 1.0\nstages.3.0.large_kernel.small_conv.bn.bias density is 1.0\nstages.3.0.norm.weight density is 1.0\nstages.3.0.norm.bias density is 1.0\nstages.3.0.pwconv1.weight density is 0.5056382478503422\nstages.3.0.pwconv1.bias density is 1.0\nstages.3.0.pwconv2.weight density is 0.5841641919188784\nstages.3.0.pwconv2.bias density is 1.0\nstages.3.1.gamma density is 1.0\nstages.3.1.large_kernel.LoRA1.conv.weight density is 0.011373750216725423\nstages.3.1.large_kernel.LoRA1.bn.weight density is 1.0\nstages.3.1.large_kernel.LoRA1.bn.bias density is 1.0\nstages.3.1.large_kernel.LoRA2.conv.weight density is 0.011778304340287812\nstages.3.1.large_kernel.LoRA2.bn.weight density is 1.0\nstages.3.1.large_kernel.LoRA2.bn.bias density is 1.0\nstages.3.1.large_kernel.small_conv.conv.weight density is 0.03188580015026296\nstages.3.1.large_kernel.small_conv.bn.weight density is 1.0\nstages.3.1.large_kernel.small_conv.bn.bias density is 1.0\nstages.3.1.norm.weight density is 1.0\nstages.3.1.norm.bias density is 1.0\nstages.3.1.pwconv1.weight density is 0.5258095826223314\nstages.3.1.pwconv1.bias density is 1.0\nstages.3.1.pwconv2.weight density is 0.6092186495412802\nstages.3.1.pwconv2.bias density is 1.0\nstages.3.2.gamma density is 1.0\nstages.3.2.large_kernel.LoRA1.conv.weight density is 0.01248338438421083\nstages.3.2.large_kernel.LoRA1.bn.weight density is 1.0\nstages.3.2.large_kernel.LoRA1.bn.bias density is 1.0\nstages.3.2.large_kernel.LoRA2.conv.weight density is 0.010379702941686412\nstages.3.2.large_kernel.LoRA2.bn.weight density is 1.0\nstages.3.2.large_kernel.LoRA2.bn.bias density is 1.0\nstages.3.2.large_kernel.small_conv.conv.weight density is 0.03329827197595792\nstages.3.2.large_kernel.small_conv.bn.weight density is 1.0\nstages.3.2.large_kernel.small_conv.bn.bias density is 1.0\nstages.3.2.norm.weight density is 1.0\nstages.3.2.norm.bias density is 1.0\nstages.3.2.pwconv1.weight density is 0.5095631762044885\nstages.3.2.pwconv1.bias density is 1.0\nstages.3.2.pwconv2.weight density is 0.5848916576962351\nstages.3.2.pwconv2.bias density is 1.0\nnorm.weight density is 1.0\nnorm.bias density is 1.0\nhead.weight density is 1.0\nhead.bias density is 1.0\nTest:  [  0/105]  eta: 0:01:58  loss: 1.5693 (1.5693)  acc1: 61.4583 (61.4583)  acc5: 87.5000 (87.5000)  time: 1.1276  data: 0.4827  max mem: 6294\nTest:  [ 10/105]  eta: 0:00:13  loss: 1.6251 (1.6817)  acc1: 61.4583 (58.9962)  acc5: 87.5000 (86.0795)  time: 0.1453  data: 0.0440  max mem: 6294\nTest:  [ 20/105]  eta: 0:00:08  loss: 1.6294 (1.6753)  acc1: 60.4167 (58.7302)  acc5: 86.4583 (86.1607)  time: 0.0460  data: 0.0002  max mem: 6294\nTest:  [ 30/105]  eta: 0:00:06  loss: 1.6584 (1.6863)  acc1: 58.3333 (58.5013)  acc5: 86.4583 (85.7863)  time: 0.0449  data: 0.0002  max mem: 6294\nTest:  [ 40/105]  eta: 0:00:04  loss: 1.6598 (1.6943)  acc1: 57.2917 (57.7236)  acc5: 84.3750 (85.5945)  time: 0.0448  data: 0.0002  max mem: 6294\nTest:  [ 50/105]  eta: 0:00:03  loss: 1.6722 (1.7032)  acc1: 56.2500 (57.3325)  acc5: 84.3750 (85.4167)  time: 0.0447  data: 0.0001  max mem: 6294\nTest:  [ 60/105]  eta: 0:00:02  loss: 1.6627 (1.6878)  acc1: 58.3333 (57.9064)  acc5: 86.4583 (85.8094)  time: 0.0447  data: 0.0001  max mem: 6294\nTest:  [ 70/105]  eta: 0:00:02  loss: 1.6627 (1.6936)  acc1: 57.2917 (57.7612)  acc5: 87.5000 (85.7835)  time: 0.0448  data: 0.0001  max mem: 6294\nTest:  [ 80/105]  eta: 0:00:01  loss: 1.7497 (1.7022)  acc1: 55.2083 (57.7161)  acc5: 84.3750 (85.5581)  time: 0.0448  data: 0.0001  max mem: 6294\nTest:  [ 90/105]  eta: 0:00:00  loss: 1.7033 (1.6956)  acc1: 59.3750 (57.9441)  acc5: 85.4167 (85.5769)  time: 0.0446  data: 0.0001  max mem: 6294\nTest:  [100/105]  eta: 0:00:00  loss: 1.6650 (1.6944)  acc1: 59.3750 (58.0136)  acc5: 86.4583 (85.6229)  time: 0.0444  data: 0.0001  max mem: 6294\nTest:  [104/105]  eta: 0:00:00  loss: 1.6467 (1.6905)  acc1: 59.3750 (58.0500)  acc5: 86.4583 (85.6400)  time: 0.0470  data: 0.0001  max mem: 6294\nTest: Total time: 0:00:05 (0.0567 s / it)\n* Acc@1 58.050 Acc@5 85.640 loss 1.690\nAccuracy of the network on 10000 test images: 58.05000%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}